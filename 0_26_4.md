### Feature commits:  
<details>
<summary>feat: added a new cli command to recover hp search experiments (#8149)</summary>
feat: added a new cli command to recover hp search experiments (#8149)
</details>
<details>
<summary>feat: new user management filters (#8002)</summary>
feat: new user management filters (#8002)

* feat: new user management filters

This updates the user management table to have more and clearer filter
functionality. The name filter is extracted out from the header menu
into a search bar above the table, and role and status filters are added
as well. There's a refactor of the user management table as well --
we've removed the dependency on `useSettings`, instead reading/writing
directly from tue user store. In addition, we no longer read the user
data from the user store, instead doing filtering and sorting on the
backend.

As there are other places where we have this pattern (read-only
remote data is requested only when dependent values change), we
introduce a new `useLoadable` hook which handles holding the internal
state, throwing out stale updates, and setting up cancelers while also
allowing us to use the `async/await` style we're used to. The only
oddity with the API is that the hook infers its return type from the
function passed in -- when handling errors or otherwise indicating that
the result shouldn't be loaded, the user has to return `NotLoaded`
instead of not returning anything.

* add bar margin

* rename useAsync/add documentation/clear state on reload

* clear up documentation

* remove columns from settings to prevent confusion

* rename columns to match mocks

* update refresh type

* render text with renamed columns

* fix type issues

* update api

* design feedback #1

* fix lint

* adjust dropdown sizes

* update useasync import

* add omitted design changes

* add back default widths

* adjust column widths and order

* adjust selects

* left align time relativity

* update tests
</details>
<details>
<summary>feat: add support for SCIM provisioning</summary>
feat: add support for SCIM provisioning

This commit brings the SCIM support for v0.11.2 back from the dead with
minimal changes. The only new fixes/features are conflicts now correctly
returns 409 not 500, password sync is supported, externalId is not
required, and the username/password that the IdP uses for basic auth
when talking to us is configurable.

[e2e_tests changes only]
</details>
<details>
<summary>feat: RBAC Model Registry EE features [DET-8704] (#644)</summary>
feat: RBAC Model Registry EE features [DET-8704] (#644)

RBAC Model registry EE features and tests.

[e2e_tests changes only]
</details>
<details>
<summary>feat: rbac ntsc ee (#662)</summary>
feat: rbac ntsc ee (#662)

[e2e_tests changes only]
</details>
<details>
<summary>feat: add rbac agents/slots enable/disable [DET-9156] (#751)</summary>
feat: add rbac agents/slots enable/disable [DET-9156] (#751)

in the RBAC-enabled world these calls should be permitted for
ClusterAdmin role and not look at user.Admin

[e2e_tests changes only]
</details>
<details>
<summary>feat: implement rbac for master logs and cluster usage (#745)</summary>
feat: implement rbac for master logs and cluster usage (#745)

[e2e_tests changes only]
</details>
<details>
<summary>feat: add rbac support for reading job queue (#871)</summary>
feat: add rbac support for reading job queue (#871)

based on determined-ai/determined#6996
adding:
rbac implementation for job queue read
e2e tests

[e2e_tests changes only]
</details>
<details>
<summary>feat: rbac for templates (#909)</summary>
feat: rbac for templates (#909)

[e2e_tests changes only]
</details>
<details>
<summary>feat: add rbac for strict job queue control (#927)</summary>
feat: add rbac for strict job queue control (#927)

implement RBAC for controlling job queue
give permission to ClusterAdmin role to make changes in the strict case

[e2e_tests changes only]
</details>
<details>
<summary>feat: add rbac to `api/v1/master/config` [DET-9633] (#931)</summary>
feat: add rbac to `api/v1/master/config` [DET-9633] (#931)

* feat: added migrations to add "can view master config" perm

* feat: add rbac to api/v1/master/config

* chore: added release note

[e2e_tests changes only]
</details>
<details>
<summary>feat: implement CodeSample from UI Kit [WEB-1677] (#8270)</summary>
feat: implement CodeSample from UI Kit [WEB-1677] (#8270)
</details>
<details>
<summary>feat: Add tensorboard delete command to CLI (#8227)</summary>
feat: Add tensorboard delete command to CLI (#8227)

* add tensorboard_gc.go

* remove tensorboard_gc.go

* add generated proto files

* lint

* add e2e test

* lint

* spacing

* remove tmp_path

* add release note

* add check for deleted tensorboard files

* test cleanup

* remove tensorboard launch in e2e test

* remove import

* use experiment that generates tb files and better deletion check

* change cli command to delete-files

* update doc and tests

* api delete -> deleteFiles

* fix bindings

* lint docs

* doc fix

* add det e delete-tb-files

* remove command for tensorboard cli

* update docs

* move api from tensorboard to experiment

* lint

* spacing
</details>
<details>
<summary>feat: Show "-" for null values in data cells for experiment list  (#8…</summary>
feat: Show "-" for null values in data cells for experiment list  (#8343)
</details>
<details>
<summary>feat: delete TB files from the SDK (#8329)</summary>
feat: delete TB files from the SDK (#8329)

SDK Experiment objects can now `delete_tensorboard_files`. This
(recursively) removes the directory
  <tb-root>/tensorboard/experiment/<exp-id>
on master

Also migrates the CLI's delete to use the SDK
</details>
<details>
<summary>feat: clear filter from experiment table header (#8376)</summary>
feat: clear filter from experiment table header (#8376)
</details>
<details>
<summary>feat: Cluster historical usage charts move to UI Kit LineChart [WEB-1…</summary>
feat: Cluster historical usage charts move to UI Kit LineChart [WEB-1786] [WEB-1764] (#8327)

* port previous changes
* add legends
* add comment
</details>
<details>
<summary>feat: add hide column header menu item to explist (#8342)</summary>
feat: add hide column header menu item to explist (#8342)
</details>
<details>
<summary>feat: change cli command for patch master log config DET[9720] (#8054)</summary>
feat: change cli command for patch master log config DET[9720] (#8054)

* feat: change cli command for patch master log config

* Made sugested changes and simplified code
</details>
<details>
<summary>feat: add support for SCIM provisioning</summary>
feat: add support for SCIM provisioning

This commit brings the SCIM support for v0.11.2 back from the dead with
minimal changes. The only new fixes/features are conflicts now correctly
returns 409 not 500, password sync is supported, externalId is not
required, and the username/password that the IdP uses for basic auth
when talking to us is configurable.

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: add support for SAML</summary>
feat: add support for SAML

To meet customer needs, we need to provide support for users to
authenticate with SAML. The commit adds /saml/sso and /saml/initiate to
receive SAML responses and initiate SP-initiated flow with SAML
requests, respectively. It also adds SSO providers to /info for the
frontend to determined if SAML is enabled or not and the ability to
configure SAML via master.yaml.
</details>
<details>
<summary>feat: add support for automatically storing creds</summary>
feat: add support for automatically storing creds

As a Determined user, I don't want to paste a token every time I want to
auth. This adds support for, when auth'ing from the CLI, redirecting to
localhost and automatically storing the auth token.
</details>
<details>
<summary>feat: support acting as an OAuth 2.0 authorization server (#10)</summary>
feat: support acting as an OAuth 2.0 authorization server (#10)

This adds support for the OAuth 2.0 authorization code flow to the
master. Apart from the core flow itself, HTTP endpoints are provided for
managing the set of OAuth client applications (accessible only by admin
users).

Since we're only addressing one particular scenario at the moment, we
allow only a single client application to be registered, since that's
all we need for this use case and that simplifies management a bit.
</details>
<details>
<summary>feat: enable OAuth protection for SCIM endpoints (#10)</summary>
feat: enable OAuth protection for SCIM endpoints (#10)
</details>
<details>
<summary>feat: add SaaS access control enforcement for Determined EE clusters …</summary>
feat: add SaaS access control enforcement for Determined EE clusters (#82)
</details>
<details>
<summary>feat: add OIDC support (#84)</summary>
feat: add OIDC support (#84)
</details>
<details>
<summary>feat: track EE in analytics (#86)</summary>
feat: track EE in analytics (#86)
</details>
<details>
<summary>feat: OIDC on helm (#105)</summary>
feat: OIDC on helm (#105)

* feat: allow oidc secret to be set from env variable

* allow oidc to be configured from helm
</details>
<details>
<summary>feat: allow OIDC to authenticate users using arbitrary claims</summary>
feat: allow OIDC to authenticate users using arbitrary claims

Previously, we hard-coded authentication by matching the `email` field
in the OIDC claim to the username provided through SCIM; now we allow
matching any claim from the OIDC token to any attribute from SCIM to
perform authentication.
</details>
<details>
<summary>feat: slurm support (#98)</summary>
feat: slurm support (#98)

This change adds a dispatcher resource manager, which gives Determined the ability to run on top of Slurm.

Co-authored-by: rcorujo <90728398+rcorujo@users.noreply.github.com>
Co-authored-by: Phillip Gaisford <phillip.gaisford@hpe.com>
Co-authored-by: phillip-gaisford <98362331+phillip-gaisford@users.noreply.github.com>
Co-authored-by: Jerry J. Harrow <84593277+jerryharrow@users.noreply.github.com>
Co-authored-by: Jagadeesh Madagundi <jagadeesh545@gmail.com>
Co-authored-by: Philip Norman <philipnrmn@users.noreply.github.com>
</details>
<details>
<summary>feat: Enable det shell start with podman (FOUNDENG-152) (#371)</summary>
feat: Enable det shell start with podman (FOUNDENG-152) (#371)
</details>
<details>
<summary>feat: Enable explicit port management with podman (FOUNDENG-150) (#373)</summary>
feat: Enable explicit port management with podman (FOUNDENG-150) (#373)
</details>
<details>
<summary>feat: Add support for PBS (FOUNDENG-187) (#397)</summary>
feat: Add support for PBS (FOUNDENG-187) (#397)

Add search list of both Slurm & PBS carriers to enable dynamic
selection of whichever is available on the target system.
Updated unit tests to expect two carriers.
</details>
<details>
<summary>feat: add EE portion of RBAC (#415)</summary>
feat: add EE portion of RBAC (#415)

Co-authored-by: Max Russell <max.russell@hpe.com>
</details>
<details>
<summary>feat: permission summary API and permissions + precanned roles (#426)</summary>
feat: permission summary API and permissions + precanned roles (#426)
</details>
<details>
<summary>feat: Reconnect to Slurm jobs on startup (FOUNDENG-215) (#429)</summary>
feat: Reconnect to Slurm jobs on startup (FOUNDENG-215) (#429)

We previously terminated running jobs upon a master restart.
Now that Determined core supports re-attaching to jobs, do the
same for DispatchRM.

- Change configure to indiate that DispatcherRM supports reattach
- Handle allocation messages with Restore:true
- Fail any allocations on Restore:true if the Dispatch ID is missing.
- Handle the case were we no longer have the payload name
  which was lost in the restart.   Ask the launcher for it in the
  very rare case where the job was started but fails before the
  rendezvous and we need the payload name to retrieve the logs.
- When in debug mode, defer dispatch cleanup util the next restart.
  On restart terminate all dispatches.
</details>
<details>
<summary>feat: rbac authz experiments api [DET-8207] (#434)</summary>
feat: rbac authz experiments api [DET-8207] (#434)
</details>
<details>
<summary>feat: add RBAC implementation of workspace authz (#436)</summary>
feat: add RBAC implementation of workspace authz (#436)
</details>
<details>
<summary>feat: rbac implementation for user authorization [DET-8205] (#445)</summary>
feat: rbac implementation for user authorization [DET-8205] (#445)
</details>
<details>
<summary>feat: rbac project authz implementation (#446)</summary>
feat: rbac project authz implementation (#446)
</details>
<details>
<summary>feat: auto assign workspace admin to workspace creator [DET-8212] (#440)</summary>
feat: auto assign workspace admin to workspace creator [DET-8212] (#440)
</details>
<details>
<summary>feat: get workspace assigned users and groups [DET-8442] (#444)</summary>
feat: get workspace assigned users and groups [DET-8442] (#444)
</details>
<details>
<summary>feat: ee support for agent user group settings per workspace. (#460)</summary>
feat: ee support for agent user group settings per workspace. (#460)
</details>
<details>
<summary>feat: echo auth for ee (#479)</summary>
feat: echo auth for ee (#479)
</details>
<details>
<summary>feat: Add CAN_EDIT_WEBHOOKS permission to pre-canned admin role [WEB-…</summary>
feat: Add CAN_EDIT_WEBHOOKS permission to pre-canned admin role [WEB-218] (#471)
</details>
<details>
<summary>feat: RBAC authz for user groups [DET-8477] (#473)</summary>
feat: RBAC authz for user groups [DET-8477] (#473)
</details>
<details>
<summary>feat: RBAC authz for RBAC [DET-8206] [DET-8368] (#480)</summary>
feat: RBAC authz for RBAC [DET-8206] [DET-8368] (#480)
</details>
<details>
<summary>feat: make list groups roles and list users roles return assignment i…</summary>
feat: make list groups roles and list users roles return assignment info (#498)
</details>
<details>
<summary>feat: Fully support apptainer fork of singularity [FOUNDENG-292] (#507)</summary>
feat: Fully support apptainer fork of singularity [FOUNDENG-292] (#507)

Apptainer 1.0 is a fork of Singularity 3.8.  Reduce use of SINGULARITY_* variables.
hpe-hpc-launcher 3.1.4 supports capabilities and cached bypass.   --no-mount=tmp
has been the default for a bit, so not explicitly needed.

We retain the use of the SINGULARITY_DOCKER* and add APPTAINER_DOCKER*
for creds as there is no CLI option alternative.   Adding the APPTAINER_* version
eliminates warnings.
</details>
<details>
<summary>feat: rbac permissive mode [DET-8308] (#534)</summary>
feat: rbac permissive mode [DET-8308] (#534)
</details>
<details>
<summary>feat: RBAC ee audit logging (#544)</summary>
feat: RBAC ee audit logging (#544)
</details>
<details>
<summary>feat: support container run type enroot. (#552)</summary>
feat: support container run type enroot. (#552)
</details>
<details>
<summary>feat: override autodetection of default resource pool (#564)</summary>
feat: override autodetection of default resource pool (#564)
</details>
<details>
<summary>feat: (FOUNDENG-344) Change enroot to use /var/tmp as localTmp direct…</summary>
feat: (FOUNDENG-344) Change enroot to use /var/tmp as localTmp directory (#631)

* use /var/tmp as tmpfs
Co-authored-by: Pankaj Mandal <mandalpa@cray.com>
</details>
<details>
<summary>feat: RBAC Model Registry EE features [DET-8704] (#644)</summary>
feat: RBAC Model Registry EE features [DET-8704] (#644)

RBAC Model registry EE features and tests.

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: rbac ntsc ee (#662)</summary>
feat: rbac ntsc ee (#662)

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: Model tags authz [WEB-873] / rebase (#674)</summary>
feat: Model tags authz [WEB-873] / rebase (#674)
</details>
<details>
<summary>feat: Add RBAC backend for CanMoveModel (#690)</summary>
feat: Add RBAC backend for CanMoveModel (#690)
</details>
<details>
<summary>feat: Add accelerator type for Slurm [FOUNDENG-483] (#711)</summary>
feat: Add accelerator type for Slurm [FOUNDENG-483] (#711)

If the Slrum hpcPartitionDetails contains accelerator type set it for the resource pool.
</details>
<details>
<summary>feat: collect timing data for dispatcher API calls (#734)</summary>
feat: collect timing data for dispatcher API calls (#734)
</details>
<details>
<summary>feat: agent enable and disable functionality for slurm clusters (#727)</summary>
feat: agent enable and disable functionality for slurm clusters (#727)
</details>
<details>
<summary>feat: override partition description (#741) [DET-9107]</summary>
feat: override partition description (#741) [DET-9107]

Add ability to override partition descriptions.
</details>
<details>
<summary>feat: smarter task container defaults merging (#742) [DET-9106]</summary>
feat: smarter task container defaults merging (#742) [DET-9106]

Multi-level defaulting for task container defaults should merge, not overwrite.
</details>
<details>
<summary>feat: add rbac agents/slots enable/disable [DET-9156] (#751)</summary>
feat: add rbac agents/slots enable/disable [DET-9156] (#751)

in the RBAC-enabled world these calls should be permitted for
ClusterAdmin role and not look at user.Admin

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: Add resource_manager.launcher_jvm_args option [DET-9230] (#780)</summary>
feat: Add resource_manager.launcher_jvm_args option [DET-9230] (#780)

Provide resource_manager setting to enable launcher JVM
arg configuration via master.yaml
</details>
<details>
<summary>feat: added RBAC for DeleteModel/Version [DET-9048] (#755)</summary>
feat: added RBAC for DeleteModel/Version [DET-9048] (#755)
</details>
<details>
<summary>feat: implement rbac for master logs and cluster usage (#745)</summary>
feat: implement rbac for master logs and cluster usage (#745)

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: Add resource_manager.sudo_authorized option (#856)</summary>
feat: Add resource_manager.sudo_authorized option (#856)
</details>
<details>
<summary>feat: add rbac support for reading job queue (#871)</summary>
feat: add rbac support for reading job queue (#871)

based on determined-ai/determined#6996
adding:
rbac implementation for job queue read
e2e tests

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: added rbac for viewing agent info [DET-9545] (#870)</summary>
feat: added rbac for viewing agent info [DET-9545] (#870)
</details>
<details>
<summary>feat: jwt invalidation for saas-created clusters (#838)</summary>
feat: jwt invalidation for saas-created clusters (#838)

This PR adds invalidation checking to user sessions when external
sessions are enabled. The cluster will poll the configured endpoint
every minute to collect a list of invalidated users, and will not trust
sessions which fall before the invalidation date.
</details>
<details>
<summary>feat: rbac for templates (#909)</summary>
feat: rbac for templates (#909)

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: add rbac for strict job queue control (#927)</summary>
feat: add rbac for strict job queue control (#927)

implement RBAC for controlling job queue
give permission to ClusterAdmin role to make changes in the strict case

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: add rbac to `api/v1/master/config` [DET-9633] (#931)</summary>
feat: add rbac to `api/v1/master/config` [DET-9633] (#931)

* feat: added migrations to add "can view master config" perm

* feat: add rbac to api/v1/master/config

* chore: added release note

[excluding e2e_tests changes]
</details>
<details>
<summary>feat: add rbac to patch master config api (#951)</summary>
feat: add rbac to patch master config api (#951)
</details>
<details>
<summary>feat: added rbac for oauth [DET-9581] (#944)</summary>
feat: added rbac for oauth [DET-9581] (#944)
</details>
<details>
<summary>feat: silenced rbac audit logs for CanViewSensitiveAgentInfo [DET-967…</summary>
feat: silenced rbac audit logs for CanViewSensitiveAgentInfo [DET-9672] (#954)

* feat: silenced CanViewSensitiveAgentInfo audit logs [DET-9672]

* feat: hide slot id's in agent summaries when user does not have access to sensitive agent info

* chore: added release notes

* refactor: made logging option clearer for check permissions, replaced with option struct

* refactor: consolidated release notes
</details>
<details>
<summary>feat: edit and reset raw user settings (take 2) (#956)</summary>
feat: edit and reset raw user settings (take 2) (#956)
</details>
<details>
<summary>feat: rp workspace mapping RBAC (#967)</summary>
feat: rp workspace mapping RBAC (#967)
</details>
<details>
<summary>feat: Update user.modified_at when roles are added or removed [WEB-15…</summary>
feat: Update user.modified_at when roles are added or removed [WEB-1503] (#1003)

* handled group PR in OSS
</details>
<details>
<summary>feat: check if default RPs are bound (EE) (#1011)</summary>
feat: check if default RPs are bound (EE) (#1011)
</details>
<details>
<summary>feat: ee rp bindings rbac permission (#1017)</summary>
feat: ee rp bindings rbac permission (#1017)
</details>
<details>
<summary>feat: fetch, store and provide external job details as part of the Di…</summary>
feat: fetch, store and provide external job details as part of the Dispatcher RM. (#855)
</details>
<details>
<summary>feat: auto provision users & sync (#1113)</summary>
feat: auto provision users & sync (#1113)

OIDC users can be auto-provisioned upon their first login.
</details>
<details>
<summary>feat: client gets list_models, too. (#8425)</summary>
feat: client gets list_models, too. (#8425)

When we added `list_models` to `Determined`, we forgot to add it to
`client`. Meant that users got `Determined.get_models` deprecation
notices (via `client.get_models`) that they couldn't do anything about.

MLG-1301

(cherry picked from commit 2e0a5a2d8682b73a07ced9fa1be0b1d320aeb970)
</details>
<details>
<summary>feat: add workspace/project creation/deletion (#8430)</summary>
feat: add workspace/project creation/deletion (#8430)

* Create a Workspace from client / Determined
* Delete a Workspace from client / Determined
* Create a Project from Workspace
* Delete a Project from Workspace

(cherry picked from commit aa34aa72d1f6e92de3479ad90834dcbbe79193da)
</details>
### Bug fix commits:  
<details>
<summary>fix: prevent settings store from triggering rerenders on poll (#8295)</summary>
fix: prevent settings store from triggering rerenders on poll (#8295)

* fix: prevent settings store from triggering rerenders on poll

this fixes an issue where components that used the settings store would
rerender on poll even if the polling function returned identical data.
This is slightly different from #8212 -- there, the entire settings
object was recreated, causing anything that relied on objects within the
settings to refresh. This fixes the issue where each render returned the
same data in a new Loadable.

* add testing
</details>
<details>
<summary>fix: undefined handling in `CreateGroupModal` (#8301)</summary>
fix: undefined handling in `CreateGroupModal` (#8301)
</details>
<details>
<summary>fix: check externalConfig is enabled before setting det_jwt as auth h…</summary>
fix: check externalConfig is enabled before setting det_jwt as auth header (#8298)
</details>
<details>
<summary>fix: adjust width size in group table (#8309)</summary>
fix: adjust width size in group table (#8309)
</details>
<details>
<summary>fix: e2e_test test_slurm.py test_node_not_available fails on CPU base…</summary>
fix: e2e_test test_slurm.py test_node_not_available fails on CPU based cluster (Mosaic) due to different Error output (FOUNDENG-132)  (#364)

[FOUNDENG-132]
Added extra error logs to account for differences with clusters with no GPUs. Also updated test_docker_login to include checks for error logs that are due to docker download rate limitations.
</details>
<details>
<summary>fix: FOUNDENG-303 Pausing, then resuming an experiment fails (#533)</summary>
fix: FOUNDENG-303 Pausing, then resuming an experiment fails (#533)
</details>
<details>
<summary>fix: FOUNDENG-310 test_noop_pause_hpc needs timeout increase to avoid…</summary>
fix: FOUNDENG-310 test_noop_pause_hpc needs timeout increase to avoid random failures (#539)
</details>
<details>
<summary>fix: stop printing incorrect (exit code 1) for failed command. (#588)</summary>
fix: stop printing incorrect (exit code 1) for failed command. (#588)

[e2e_tests changes only]
</details>
<details>
<summary>fix: Reduce verbosity of failure messages [FOUNDENG-370] (#583)</summary>
fix: Reduce verbosity of failure messages [FOUNDENG-370] (#583)

Optimize the error messages on failure to reduce duplication.
Send the error.log portion only to the task log directly
so that it is not duplicated in the master log.

[e2e_tests changes only]
</details>
<details>
<summary>fix: Improve test_noop_hpc reliability [FOUNDENG-361] (#590)</summary>
fix: Improve test_noop_hpc reliability [FOUNDENG-361] (#590)

The test_noop_hpc test pauses/activates an experiment.
Sometimes the test reaches COMPLETE before the PAUSE happens, causing the test
to fail.  Double the batches to hopefully get the test to succeed more reliably.
</details>
<details>
<summary>fix: FOUNDENG-336 test_noop_hpc continues to fail periodically (#632)</summary>
fix: FOUNDENG-336 test_noop_hpc continues to fail periodically (#632)
</details>
<details>
<summary>fix: ntsc endpoints should return 404 on unauthorized workspace id [d…</summary>
fix: ntsc endpoints should return 404 on unauthorized workspace id [det-8911] (#671)

* add tests for list endpoint behavior

* avoid mapping known err to internal
</details>
<details>
<summary>fix: tensorboard list not showing tensorboards [DET-8904] (#669)</summary>
fix: tensorboard list not showing tensorboards [DET-8904] (#669)

[e2e_tests changes only]
</details>
<details>
<summary>fix: websocket upgrade failed in tensorboard [DET-8903] (#672)</summary>
fix: websocket upgrade failed in tensorboard [DET-8903] (#672)

[e2e_tests changes only]
</details>
<details>
<summary>fix: user can only list models with correct permissions + small fixes…</summary>
fix: user can only list models with correct permissions + small fixes in workspace filtering in get models (#681)

[e2e_tests changes only]
</details>
<details>
<summary>fix: Fail attempts to mount under /run/determined on HPC [FOUNDENG-48…</summary>
fix: Fail attempts to mount under /run/determined on HPC [FOUNDENG-482] (#710)

Singularity/Enroot do not support overlapping mounts.   Allowing a user
mount under /run/determined conflicts with generated mounts, so fail
such attempts with a clean error message.

[e2e_tests changes only]
</details>
<details>
<summary>fix: rbac e2e test (#738)</summary>
fix: rbac e2e test (#738)

[e2e_tests changes only]
</details>
<details>
<summary>fix: fix bug with launching tensorboards on trials (#842)</summary>
fix: fix bug with launching tensorboards on trials (#842)

fix SQL query causing tensorboards with explicitly requested trials to fail to launch.

[e2e_tests changes only]
</details>
<details>
<summary>fix: Patch groups test [DET-9473] (#845)</summary>
fix: Patch groups test [DET-9473] (#845)

* use correct mark
</details>
<details>
<summary>fix: DET-9483 successfully run e2e_slurm_preemption tests as part of …</summary>
fix: DET-9483 successfully run e2e_slurm_preemption tests as part of nightly workflow (#903)

[e2e_tests changes only]
</details>
<details>
<summary>fix: test_rbac goes to wrong url (#918)</summary>
fix: test_rbac goes to wrong url (#918)
</details>
<details>
<summary>fix: Test test_slurm_verify_home fails with podman and it shouldn't […</summary>
fix: Test test_slurm_verify_home fails with podman and it shouldn't [FE-136] (#1028)

Removed the skipping of 'test_slurm_verify_home' for podman from .circleci/config.yml [ALLGCP]

[e2e_tests changes only]
</details>
<details>
<summary>fix: update for error message change in product (#1098)</summary>
fix: update for error message change in product (#1098)

* fix: update for error message change in product
</details>
<details>
<summary>fix: ruamel.yaml fixes for EE</summary>
fix: ruamel.yaml fixes for EE
</details>
<details>
<summary>fix: return correct location URL for /Users SCIM API endpoint (#1115)</summary>
fix: return correct location URL for /Users SCIM API endpoint (#1115)

[e2e_tests changes only]
</details>
<details>
<summary>fix: Project info not presists when forking (#8307)</summary>
fix: Project info not presists when forking (#8307)
</details>
<details>
<summary>fix: allow experiments with directory checkpoint storage to parse (#8…</summary>
fix: allow experiments with directory checkpoint storage to parse (#8310)
</details>
<details>
<summary>fix: metric group charts have more than one color (#8304)</summary>
fix: metric group charts have more than one color (#8304)
</details>
<details>
<summary>fix: add pin icon in dropdown (#8324)</summary>
fix: add pin icon in dropdown (#8324)
</details>
<details>
<summary>fix: cli is not a library! (#7891)</summary>
fix: cli is not a library! (#7891)

Direct calls into the CLI are problematic, because the CLI uses a
variety of authentication techniques, some of which are only configured
by the cli's __main__() codepath.  That means that cli auth and cert
singletons have been configured all over e2e tests in order to support
direct CLI calls.

Avoiding direct calls to cli functions is a step in the direction of
cleaning up our authentication story.
</details>
<details>
<summary>fix: replace TODO with ctx for deleteTensorboard (#8332)</summary>
fix: replace TODO with ctx for deleteTensorboard (#8332)
</details>
<details>
<summary>fix: Allow SAML and OIDC logins to work differently [WEB-1797] (#8308)</summary>
fix: Allow SAML and OIDC logins to work differently [WEB-1797] (#8308)

* fix: web UI receives SSO type
</details>
<details>
<summary>fix: properly interpret flag values (#8326)</summary>
fix: properly interpret flag values (#8326)

Both `--preemptible` and `--preemption-enabled` cast values with the
built-in `bool`, which casts to True any possible value but "0". Even
"false" and "False". This is almost certainly not what users expect.

Using `string_to_bool`, "0" is still supported. But now all the strings
that should evaluate to False do, too.
</details>
<details>
<summary>fix(experiments): transient errors shouldn't leave trial hung (#8352)</summary>
fix(experiments): transient errors shouldn't leave trial hung (#8352)

`handleAllocationExit` has an unspoken invariant that it must further
the state of the trial somehow, either by closing it or trying to continue
it. This change fixes bugs where this wasn't upheld and make it clearer
that it must be in the function comment.
</details>
<details>
<summary>fix: Set group name and number columns to handle Safari [DET-9948] [D…</summary>
fix: Set group name and number columns to handle Safari [DET-9948] [DET-9949] (#8355)

---------

Co-authored-by: Keita Nonaka <keita.nonaka@hpe.com>
</details>
<details>
<summary>fix: prevent carriage return in env from crashing deepspeed launcher …</summary>
fix: prevent carriage return in env from crashing deepspeed launcher (#8321)
</details>
<details>
<summary>fix: patched remote users were able to login with password (#8337)</summary>
fix: patched remote users were able to login with password (#8337)
</details>
<details>
<summary>fix(tests): lower e2e_gpu_quarantine parallelism (#8363)</summary>
fix(tests): lower e2e_gpu_quarantine parallelism (#8363)

e2e_gpu_quarantine is failing on main because one of the splits has no
tests, but there is only one test to run so we don't need splits at all. I
could fix the shell to not panic if we run a useless split, but I think it
should still, honestly. That's a lot of wasted resources.
</details>
<details>
<summary>fix: Cell can be undefined in experiment list table (#8360)</summary>
fix: Cell can be undefined in experiment list table (#8360)
</details>
<details>
<summary>fix: fixed bug in error handling in experiment.go (#8339)</summary>
fix: fixed bug in error handling in experiment.go (#8339)
</details>
<details>
<summary>fix: aws deployment can deploy priority scheduler (#8345)</summary>
fix: aws deployment can deploy priority scheduler (#8345)

Deployment failed under `--scheduler=priority` due to a broken reference
in the template. This fixes that.

[DET-9941]
</details>
<details>
<summary>fix: adjust card size on workspaces page (#8370)</summary>
fix: adjust card size on workspaces page (#8370)
</details>
<details>
<summary>fix: NTSC task and slot viewing obscured for RBAC users with no Viewe…</summary>
fix: NTSC task and slot viewing obscured for RBAC users with no Viewer Permissions (#8311)
</details>
<details>
<summary>fix: Project and Workspace cards wrap modal divs (#8378)</summary>
fix: Project and Workspace cards wrap modal divs (#8378)
</details>
<details>
<summary>fix: user flag to prompt for password during user requests (#8158)</summary>
fix: user flag to prompt for password during user requests (#8158)
</details>
<details>
<summary>fix: added permission check on GetAllocation (#8281)</summary>
fix: added permission check on GetAllocation (#8281)
</details>
<details>
<summary>fix: Hide stats card when 0 on cluster page (#8359)</summary>
fix: Hide stats card when 0 on cluster page (#8359)
</details>
<details>
<summary>fix: Place modal inside of ResourcePoolCard (#8414)</summary>
fix: Place modal inside of ResourcePoolCard (#8414)
</details>
<details>
<summary>fix: allow --json in det master config CLI command (#8413)</summary>
fix: allow --json in det master config CLI command (#8413)
</details>
<details>
<summary>fix: ignore SCIM meta field</summary>
fix: ignore SCIM meta field

When we receive a PATCH or PUT from Okta,
they may reply to us with the meta field we
initially gave to them. This is ok; it is in
spec to receive and ignore it.
</details>
<details>
<summary>fix: respect password sync in PUT requests</summary>
fix: respect password sync in PUT requests

When the SCIM client attempts to sync a password in the PUT request, as
in the case where a user updates or has their password reset, we should
respect it (because okta sends it).
</details>
<details>
<summary>fix: remove det-deploy local from ee ci (#53)</summary>
fix: remove det-deploy local from ee ci (#53)
</details>
<details>
<summary>fix: add ssoProviders to default store info for ee</summary>
fix: add ssoProviders to default store info for ee
</details>
<details>
<summary>fix: update master GoReleaser config to account for new binary</summary>
fix: update master GoReleaser config to account for new binary
</details>
<details>
<summary>fix: allow CLI to log in via OIDC (#92)</summary>
fix: allow CLI to log in via OIDC (#92)

* Show OIDC in sso_providers, if configured

Previously the OIDC configuration was not displayed in the /info
endpoint as a member of sso_providers, even if enabled. This meant that
the CLI could not detect it and use it to log in.

* Pass relayState through OIDC auth flow

In the SAML flow, relayState is passed through the authentication flow
via redirect binding. This was naively copied into OIDC, but as it isn't
part of the OIDC standard it didn't work.  Here we simply add the
relayState to the redirect URI's query string.
</details>
<details>
<summary>fix: update docs skip check remote address to EE</summary>
fix: update docs skip check remote address to EE
</details>
<details>
<summary>fix: compensate for breaking change #4460 (#326)</summary>
fix: compensate for breaking change #4460 (#326)

* fix: compensate for breaking change #4460
</details>
<details>
<summary>fix: Cleanup CPU-only system error reporting (FOUNDENG-117) (#335)</summary>
fix: Cleanup CPU-only system error reporting (FOUNDENG-117) (#335)

Ensure that the extended error messages are reported on submission failure by expanding the pattern.

Suppress environment cleanup on LeveDebug and greater as LevelTrace is kind of unusable due to the amount of output logged.
</details>
<details>
<summary>fix: Exported functions (e.g. which) may break experiments (FOUNDENG-…</summary>
fix: Exported functions (e.g. which) may break experiments (FOUNDENG-145) (#351)

Bash-exported functions are set as environment variables and by default
are inherited into singularity containers.   On some systems the which
command is configured this way and injects arguments into the which
command.  When invoked inside of a determined environment image the
which command does not support these arguments and it breaks the check
for the python3 being on the path, thus breaking most experiments.

Clear all exported functions to avoid this potential collision.
</details>
<details>
<summary>fix: skip DET special ports in config podman port mappings (FE-163) (…</summary>
fix: skip DET special ports in config podman port mappings (FE-163) (#379)
</details>
<details>
<summary>fix: provide better message when failed to fetch resource pool detail…</summary>
fix: provide better message when failed to fetch resource pool details. (#398)
</details>
<details>
<summary>fix: Workaround rocm-smi python issue (FOUNDENG-127) (#403)</summary>
fix: Workaround rocm-smi python issue (FOUNDENG-127) (#403)

rocm-smi does not work within a singularity container when the host is RHEL
and the container is Ubuntu.   This is a workaround to that incompatibility.
</details>
<details>
<summary>fix: Allow over-mounting of /tmp/work (FOUNDENG-205) (#410)</summary>
fix: Allow over-mounting of /tmp/work (FOUNDENG-205) (#410)

With Singularity /tmp is removed and re-linked to a user directory
to avoid the default host-wide share /tmp and provide more space that
the limited Singularity tmpfs space (10mb).  Make the removal of /tmp
handle injected sub-directories by bind mounts, by detecting the
error and reporting an ERROR message instead of failing.

Also add a FATAL error message giving context if an error in the shell
script is terminated due to non-zero exit (set -e).
</details>
<details>
<summary>fix: remove slurm-resources-info file on job cleanup (#411)</summary>
fix: remove slurm-resources-info file on job cleanup (#411)

* fix: remove slurm-resource-info file on job cleanup

* fix: remove slurm-resources-info file on job cleanup
</details>
<details>
<summary>fix: Properly pass along PBS queue to launcher (FE-202) (#413)</summary>
fix: Properly pass along PBS queue to launcher (FE-202) (#413)

Small fix to pass along the resource_pool name as the PBS/Slurm
queue (partition is only supported by by Slurm, but PBS/Slurm both
support queue).
</details>
<details>
<summary>fix: Drop unused resource tracking data (FOUNDENG-215) (#419)</summary>
fix: Drop unused resource tracking data (FOUNDENG-215) (#419)

DispatcherRM has been maintaining data resource mapping data that has been unused since it migrated into the DB.   Drop the fields we do not need.
</details>
<details>
<summary>fix: Generalize launcher prefixes for PBS (#432)</summary>
fix: Generalize launcher prefixes for PBS (#432)

Some messages include Slurm/PBS and the carrier name.   Generalize the regex to allow either Slurm/PBS so that message processing will be handled similarly.
</details>
<details>
<summary>fix: Avoid using multiple carriers on failure we retry (FOUNDENG-232)…</summary>
fix: Avoid using multiple carriers on failure we retry (FOUNDENG-232) (#433)

We have different carriers for Slurm/PBS, but if we list them both
and the user job fails, it tries the next.   Use the dispatcherRM
wlmType to specify the carrier in use to avoid this fallback.
</details>
<details>
<summary>fix: nil ptr on protoing and workspace permission missing from viewer…</summary>
fix: nil ptr on protoing and workspace permission missing from viewer (#438)
</details>
<details>
<summary>fix: Initial prep_container should fail quickly (FOUNDENG-217) (#435)</summary>
fix: Initial prep_container should fail quickly (FOUNDENG-217) (#435)

Fix the exception rasied when master is not reacable (MasterNotFoundException)
APIHttpError only happens when call completes without a successful status
response.

The initial prep_container was ntended to fail reasonably quickly
to enable diagnostics of misconfiguration.   Recent changes
for re-using the common session (DET-8003) settings has caused the initial
communication to retry for more than 30 mins thus defeating the
original intent of prep_container.trial_prep and the error message
it provides.

This change lowers the session retry in prep_container (6 retries with 0.5
backoff -> 64 seconds) to enable the diagnostic message to be posted reasonably quickly.
</details>
<details>
<summary>fix: Wait for termination before deleting dispatch (FOUNDENG-217) (#442)</summary>
fix: Wait for termination before deleting dispatch (FOUNDENG-217) (#442)

When killing a dispatch, we are not able to immediately delete the
dispatch because the files may still be in use by the running container.
Wait until we get to a terminal state before performing the delete.
</details>
<details>
<summary>fix: Starting state now shows Running [FOUNDENG-242] (#447)</summary>
fix: Starting state now shows Running [FOUNDENG-242] (#447)

Once the job was queue with Slurm/PBS we triggered the Starting
state.  Prior to 19.4 this use to show as QUEUED in the UI, but
now has changed to "RUNNING (PREPARING ENV)" which is not
accurate.   So map PENDING -> Assigned such that the UI
continues to show "QUEUED" until the job starts running.
</details>
<details>
<summary>fix: Improve PBS error reporting [FOUNDENG-248] (#453)</summary>
fix: Improve PBS error reporting [FOUNDENG-248] (#453)

Augment the error logs with the HTTP response value on failure.  The returned
error does not always have the underlying info (e.g. 404 Not Found).

Fix the patterns use for matching messages with PBS.   Add an entry for
Slurm (which was showing up previously because no messages matched).

Add filtering based up on the reporter to eliminate some noise that we never
want to see from the Dispatcher infrastructure.
</details>
<details>
<summary>fix: Add export PATH for PBS Carrier [FOUNDENG-266] (#474)</summary>
fix: Add export PATH for PBS Carrier [FOUNDENG-266] (#474)

We minimally need the path to be inherited into the PBS
script job such that singularity run can successfully pull
and image.   It needs /usr/sbin/ on the path, but PBS
apparently doesn't inherit the system path or any such reasonable
path.  This changes allows inheritance of all environment
variables to cover PATH, and anything else the launcher may
have added to their environ (PATH, LD_LIBRARY_PATH, etc).
</details>
<details>
<summary>fix: deal with some lint (#491)</summary>
fix: deal with some lint (#491)
</details>
<details>
<summary>fix: FOUNDENG-283 Determined UI Resource Pools page incorrectly shows…</summary>
fix: FOUNDENG-283 Determined UI Resource Pools page incorrectly shows CPU usage (#490)
</details>
<details>
<summary>fix: Correct quoting in error message (#492)</summary>
fix: Correct quoting in error message (#492)

We have a custom bash error handler if any command returns
a non-zero.  Fix the quoting and spelling so that it actually works.
</details>
<details>
<summary>fix: Restore det shell on podman [FOUNDENG-280] (#493)</summary>
fix: Restore det shell on podman [FOUNDENG-280] (#493)

When running rootless podman, inside the container we are
root/uid=0 and that maps to the user account outside the
container.   All is fine until we attempt to ssh into the
container which actually then uses the launching username/uid.
Under normal circumstance /run/determined/ssh has only 0600
permissions for only the owning user to read, but with podman
root maps to the user/uid, so the launching user is not seen
as having access to the files.

Until we find a better solution, dynamically relax the permissions
to be a+x on the /run/determined/ssh directory path such that
the user can read /run/determined/ssh/authorized_keys and enable
ssh into the container to work proplerly.

Additionally, drop use of the podman --hostuser arg, as it doens't help
the situation and we already provide the launching user in a custom
passwd entry.
</details>
<details>
<summary>fix: remove =true from sso url querystring (#494)</summary>
fix: remove =true from sso url querystring (#494)
</details>
<details>
<summary>fix: add `ON DELETE CASCADE` for `role_assignments.group_id` column (…</summary>
fix: add `ON DELETE CASCADE` for `role_assignments.group_id` column (#501)
</details>
<details>
<summary>fix: searching roles results in 500 error (#503)</summary>
fix: searching roles results in 500 error (#503)
</details>
<details>
<summary>fix: PodMan map user to UID and GID to 0 in passwd [FOUNDENG-300] (#504)</summary>
fix: PodMan map user to UID and GID to 0 in passwd [FOUNDENG-300] (#504)

In rootless PodMan the user executes as uid/gid 0:0 inside the container
which maps to the actual launching user outside the container.  If
the entry point user is 'root' then map the agent user to 0:0 in
/run/determined/etc/passwd such that outside the container the access
is seen as the launching user.

/run/determined/etc/passwd contains a single line (written by Determined)
to represent the agent user.
</details>
<details>
<summary>fix: add sso login routes to list of echo routes that don't require a…</summary>
fix: add sso login routes to list of echo routes that don't require auth (#509)

Co-authored-by: Addison Snelling <asnell@hpe.com>
</details>
<details>
<summary>fix: get group 500 error for rbac can't access case [DET-8588, DET-85…</summary>
fix: get group 500 error for rbac can't access case [DET-8588, DET-8589] (#506)
</details>
<details>
<summary>fix: redirect to cli relay on det auth login (#519)</summary>
fix: redirect to cli relay on det auth login (#519)
</details>
<details>
<summary>fix: 500 error for workspace membership without perms (#525)</summary>
fix: 500 error for workspace membership without perms (#525)
</details>
<details>
<summary>fix: allow workspace viewers to view roles in webui. (#530)</summary>
fix: allow workspace viewers to view roles in webui. (#530)
</details>
<details>
<summary>fix: persistence of dispatch ID. (#538)</summary>
fix: persistence of dispatch ID. (#538)
</details>
<details>
<summary>fix: make role assignment respect global-only (#542)</summary>
fix: make role assignment respect global-only (#542)
</details>
<details>
<summary>fix: job fails due to "/var/tmp" being mounted [FOUNDENG-352] (#562)</summary>
fix: job fails due to "/var/tmp" being mounted [FOUNDENG-352] (#562)

When adding Enroot support (FOUNDENG-334), we moved the temp
per-container link folder for Singularity/PodMan from / to /var/tmp.
Singularity also specifies --pwd /var/tmp which apparently
conflicts with --nomount tmp (which is documented to avoid mounting
/tmp & /var/tmp from the host) and thereby causes /var/tmp to be
mounted from the host node.   So restore Singularity/PodMan to use /
</details>
<details>
<summary>fix: Fix build break after oss merge (#565)</summary>
fix: Fix build break after oss merge (#565)

Fix DispatcherRM reference to TaskContainerDefaults Slurm/Pbs args
Fix unit tests for new default pool configuration.
</details>
<details>
<summary>fix: adjust dispatcherrm to satisfy new interface  (#570)</summary>
fix: adjust dispatcherrm to satisfy new interface  (#570)
</details>
<details>
<summary>fix: PBS does not appear to use multiple GPUs per node [FOUNDENG-359]…</summary>
fix: PBS does not appear to use multiple GPUs per node [FOUNDENG-359] (#568)

Unless PBS has been configured to provide CUDA_VISIBLE_DEVICES, Determined uses only a
single slot, even when specifying slots_per_node.   This change provides DET_SLOT_IDS from the Dispatcher RM
if slots_per_node is specified.   If there is no CUDA_VISIBLE_DEVICES value,
DET_SLOT_IDS is now honored (if present) to enable use of the specified number of slots.

We additionally need user documentation in the PBS requirements.
</details>
<details>
<summary>fix: add determined context to fix e2e-longrunning (#572)</summary>
fix: add determined context to fix e2e-longrunning (#572)
</details>
<details>
<summary>fix: Define ENROOT_RUNTIME_PATH only for enroot (#581)</summary>
fix: Define ENROOT_RUNTIME_PATH only for enroot (#581)

Singularity cannot have executable code in the -vars.sh file.
So only define ENROOT_RUNTIME_PATH=/tmp/\$\$(whoami)
when enroot is in use.
</details>
<details>
<summary>fix: stop printing incorrect (exit code 1) for failed command. (#588)</summary>
fix: stop printing incorrect (exit code 1) for failed command. (#588)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Reduce verbosity of failure messages [FOUNDENG-370] (#583)</summary>
fix: Reduce verbosity of failure messages [FOUNDENG-370] (#583)

Optimize the error messages on failure to reduce duplication.
Send the error.log portion only to the task log directly
so that it is not duplicated in the master log.

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Suppress trail log message if only whitespace [FOUNDENG-380] (#593)</summary>
fix: Suppress trail log message if only whitespace [FOUNDENG-380] (#593)

Do not post a message of solely whitespace to the experiment log.
The HPC Dispatch monitor sends a newline on an empty message, so
ignore it if that is all that is there.
</details>
<details>
<summary>fix: assign launcher-provided pools to agents (#594)</summary>
fix: assign launcher-provided pools to agents (#594)
</details>
<details>
<summary>fix: Enable slots_per_node with gpu_type [FOUNDENG-401] (#623)</summary>
fix: Enable slots_per_node with gpu_type [FOUNDENG-401] (#623)

The tres_enabled:true case of slots_per_node is handled
in the dispatcher_task (not launcher).   When a gpu_type is
specified in a --gpus expression, it must also be applied
to the --gpus-per-task expression.

TODO:  Better if this were handled on the launcher side [FOUNDENG-403]

Tested on the grenoble system which surfaced the problem.
</details>
<details>
<summary>fix: Enable IB transport and MPI images to work by default [FOUNDENG-…</summary>
fix: Enable IB transport and MPI images to work by default [FOUNDENG-412] (#628)

The default "max locked memory" in our containers is 64, which is too low for IB
If we are getting the default value in the container, then raise it to
unlimited.   If it is set to some other value, leave it as-is to
allow a customer override.

If no user SLURM_MPI_TYPE is defined, set it to pmi2 as needed by the
Determined MPI environment image.
</details>
<details>
<summary>fix: respect expirations, support client credentials flow (#614)</summary>
fix: respect expirations, support client credentials flow (#614)
</details>
<details>
<summary>fix: tensorboard list not showing tensorboards [DET-8904] (#669)</summary>
fix: tensorboard list not showing tensorboards [DET-8904] (#669)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: websocket upgrade failed in tensorboard [DET-8903] (#672)</summary>
fix: websocket upgrade failed in tensorboard [DET-8903] (#672)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: FOUNDENG-448 adaptive searches can cause overload the launcher (…</summary>
fix: FOUNDENG-448 adaptive searches can cause overload the launcher (#677)
</details>
<details>
<summary>fix: FOUNDENG-448 adaptive searches can cause overload the launcher (…</summary>
fix: FOUNDENG-448 adaptive searches can cause overload the launcher (follow-up changes) (#689)
</details>
<details>
<summary>fix: Drop incorrect max_slots support code [FOUNDENG-454] (#691)</summary>
fix: Drop incorrect max_slots support code [FOUNDENG-454] (#691)

The expconf resources.max_slots attribute is documented as managed by
Slurm, but there was a partial (non-functional) implementation in
DispatchRM as well.  If we ever submitted max_slots worth of work
the remainder would remain QUEUED forever.   I did attempt to fix
the existing support, but the obvious fixes were not sufficient.  Since
this is already documented as managed by Slurm, just remove the support
and let the workload managers manage it.
</details>
<details>
<summary>fix: user can only list models with correct permissions + small fixes…</summary>
fix: user can only list models with correct permissions + small fixes in workspace filtering in get models (#681)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Revert back to LaunchAsync for Slurm jobs [FOUNDENG-465] (#696)</summary>
fix: Revert back to LaunchAsync for Slurm jobs [FOUNDENG-465] (#696)

With synchronous launch the jobs is fully created (and potentially running)
before we get back the dispatchID and associate it with the allocation id.
If that happens we cannot associate the NotifyContainerRunning event with
the dispatch ID and we leave the container in PULLING state.
</details>
<details>
<summary>fix: Quick fix to reduce potential race conditions [FOUNDENG-447] (#698)</summary>
fix: Quick fix to reduce potential race conditions [FOUNDENG-447] (#698)

The launcher infrastructure uses file/directory management to track dispatch state and it has some design deficiencies that we has tickets to address.     We have previously been using a fixed name ("det") and
unspecified version ("unknown") which causes all dispatches from a user to end up in the same directories.    When there are lots of jobs coming and going we have observed file conflicts that causes data loss.

By providing a generated UUID as the version string, all dispatches for a user will be in a separate directories and therefore it should drastically reduce the potential for race conditions on file management until we can fix the launcher internals.
</details>
<details>
<summary>fix: FOUNDENG-463 Need to handle when job is canceled before launcher…</summary>
fix: FOUNDENG-463 Need to handle when job is canceled before launcher creates dispatchID (#699)
</details>
<details>
<summary>fix: Avoid startup lag due to debug mode cleanup [FOUNDENG-477] (#707)</summary>
fix: Avoid startup lag due to debug mode cleanup [FOUNDENG-477] (#707)

DispatcherRM debug mode defers cleanup of dispatches to enable access to the detailed
job logs to enable triage when things are not working.   Because it was intended for internal debugging
only, it had not been optimized and did the cleanup of the dispatches synchronously.  This blocks
startup of the Determined master.   Enhance the debug mode cleanup to be done in a go routine, and
only cleanup dispatches associated with completed allocation (previously it cleaned up
everything and forced use of a restart).   This change should make it reasonable for
use on a customer site when necessary.
</details>
<details>
<summary>fix: Handle subdirs in expconf model_def with Launcher [FOUNDENG-479]…</summary>
fix: Handle subdirs in expconf model_def with Launcher [FOUNDENG-479] (#709)

If a model_def directory contained a subdir, it was incorrectly treated
as needing a softlink (but that is needed only for the top-level
child dirs of /run/determined) which causes a failure in the launcher
when trying to expand the archive.  Only convert top-level paths
under /run/determined to softlinks.
</details>
<details>
<summary>fix: slurmcluster uses wrong host to kill sshd procs for the user (#706)</summary>
fix: slurmcluster uses wrong host to kill sshd procs for the user (#706)
</details>
<details>
<summary>fix: Fail attempts to mount under /run/determined on HPC [FOUNDENG-48…</summary>
fix: Fail attempts to mount under /run/determined on HPC [FOUNDENG-482] (#710)

Singularity/Enroot do not support overlapping mounts.   Allowing a user
mount under /run/determined conflicts with generated mounts, so fail
such attempts with a clean error message.

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: FOUNDENG-480 Allow KillDispatcherResources messages to be handle…</summary>
fix: FOUNDENG-480 Allow KillDispatcherResources messages to be handled in parallel (#719)
</details>
<details>
<summary>fix: Unexpected unmarshall error when exp config is empty (#721)</summary>
fix: Unexpected unmarshall error when exp config is empty (#721)
</details>
<details>
<summary>fix: update dispatcher RM unit tests after omitting fields (#728)</summary>
fix: update dispatcher RM unit tests after omitting fields (#728)
</details>
<details>
<summary>fix: add permission check for tensorboard workspace (#730)</summary>
fix: add permission check for tensorboard workspace (#730)
</details>
<details>
<summary>fix: FOUNDENG-508 Slurm GRES causes checkpoint GC task error: sbatch:…</summary>
fix: FOUNDENG-508 Slurm GRES causes checkpoint GC task error: sbatch: error: Invalid numeric value "0" [DET-9014] (#732)
</details>
<details>
<summary>fix: filter by username and display name in users by workspace assign…</summary>
fix: filter by username and display name in users by workspace assignment (#733)
</details>
<details>
<summary>fix: Update launcher dependency check [FOUNDENG-512] (#736)</summary>
fix: Update launcher dependency check [FOUNDENG-512] (#736)

Determined master now requires launcher 3.2.3.
</details>
<details>
<summary>fix: envvars without a value do not work [DET-9121] (#747)</summary>
fix: envvars without a value do not work [DET-9121] (#747)

DispatcherRM explicitly failed on envars without a value.
Instead pass them along to the launcher.  Update the unit tests.
</details>
<details>
<summary>fix: Properly honor default resource pool [DET-9116] (#746)</summary>
fix: Properly honor default resource pool [DET-9116] (#746)

- Properly set the default attribute on any custom HPC resource pools
that are referenced as the default when returning GUI cluster data.
- Fix ResolveResourcePool to process the pool in the normal manner
when infering the default.   Previously it assumed that the returned
name was always a partition.
- Add support in slurmcluster for custom hpc resource pools, and define
one for casablanca for testing.
- Add a partition override pool description for casablanca which will
demonstrate overriding pool descriptions when they are checked in.
</details>
<details>
<summary>fix: rbac e2e test (#738)</summary>
fix: rbac e2e test (#738)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Cleanup dispatches associated with terminated allocations on sta…</summary>
fix: Cleanup dispatches associated with terminated allocations on startup [DET-9142] (#750)

On startup active allocations are restored, but for those that are already terminated (for whatever reason) we should cleanup any associated dispatches.   Previously, this code was only used on debug, but if we ever end up in the case where the DispatcherRM failed to handle the allocation termination (due to a bug for example), this will be sure the allocations move from canceling->canceled.
</details>
<details>
<summary>fix: FOUNDENG-510 Ensure canceled experiments/trials are properly ter…</summary>
fix: FOUNDENG-510 Ensure canceled experiments/trials are properly terminated (#748)

* fix: FOUNDENG-510 Need to handle when job fails before the dispatch RM gets the HPC Job ID

* Better handle conflicting job termination retries

* Fix unit tests

* Additional logging for the retries

* Removed code that was no longer necessary now that launcher side FOUNDENG-518 and FOUNDENG-520 have been fixed

* fix: FOUNDENG-510 Ensure canceled experiments/trials are properly terminated

* Removed additional code that relied on HPC job ID, but we no longer need

* Fixed failing unit test that checks for minimum launcher version

* Remove our retries for failed cancelations

* Addressed reviewer comment to use WithField() for dispatchID

* Removed 404 retries, as they are no longer necessary now that the launcher creates the environment before returning the dispatch ID

* Reversed the meaning of the boolean value returned by getDispatchStatus(), as per reviewer request
</details>
<details>
<summary>fix: Launcher DeleteEnvironment causes DispatchRM lockup [DET-9157] (…</summary>
fix: Launcher DeleteEnvironment causes DispatchRM lockup [DET-9157] (#752)

Deletion of the dispatch is done synchronously in the DispatchExited event handler, need to make it async to avoid blocking the event handler.

Moved content of DispatchExited to a go routine (dispatchExited) except for accesses to m.reqList which should remain to avoid the need for additional synchronization.
Identified one other synchronous call to m.removeDispatchEnvironment that needed to be made async.
Added comments to other call sites indicating they are already invoked from an existing go routine so are non-blocking.

Extracted the m.reqList use from startLauncherJob go routine back into the event handler to avoid the need for additional synchronization.
</details>
<details>
<summary>fix: get 'slot list' going for dispatcher (#758)</summary>
fix: get 'slot list' going for dispatcher (#758)
</details>
<details>
<summary>fix: Properly handle a "--wckey" value in sbatch file that has spaces…</summary>
fix: Properly handle a "--wckey" value in sbatch file that has spaces [DET-9199] (#765)

Switched to using strconv.Quote() to add the double quotes and escape embedded quotes, as Bradley had suggested

If label is empty, then wckey will be "". Otherwise, PBS fails if there's a '-P' with nothing after it.
</details>
<details>
<summary>fix: Use synchronized method to access dispatchIDToHPCJobID [DET-9222…</summary>
fix: Use synchronized method to access dispatchIDToHPCJobID [DET-9222] (#775)

The dispatcher_monitor is reaching inside the dispatcher_resource_manager and accessing dispatchIDToHPCJobID without a lock.
If it is concurrent with another access it crashes the master.
Use the proper method which provides locking to avoid the crash..
</details>
<details>
<summary>fix: EE filter experiment list - bug fix (#766)</summary>
fix: EE filter experiment list - bug fix (#766)

* filter testing
* use a bun query to return workspace IDs with the relevant permissions
</details>
<details>
<summary>fix: no warning if user did not config priority (#776)</summary>
fix: no warning if user did not config priority (#776)
</details>
<details>
<summary>fix: duplicate migration idempotently for clusters that ran it out-of…</summary>
fix: duplicate migration idempotently for clusters that ran it out-of-order (#769)
</details>
<details>
<summary>fix: Improve error message for EOF error response (#782)</summary>
fix: Improve error message for EOF error response (#782)
</details>
<details>
<summary>fix: use time.Second for time interval in dispatcher version check (#…</summary>
fix: use time.Second for time interval in dispatcher version check (#786)
</details>
<details>
<summary>fix: SCIM users are remote, by default (#783)</summary>
fix: SCIM users are remote, by default (#783)

We had an issue where users remote that were created through SCIM
or the regular user system could call `det user change-password` to
begin using password login. Remote users created through SCIM should
never change their password, they are either 'NoPassword' users [1]
that will also have SSO configured or they have password sync enabled
in through SCIM; either way their IdP manages their password. Some
users do not use SCIM, but provision SSO users as regular Determined
users. For these users, we add `det user create --remote` to
differentiate remote managed users.

[1]. 'NoPassword' users in SCIM don't actually exist in Okta 🙃 . Even if
unselect 'Sync Passwords', Okta sends a random 8 char string in the
password field. So someone could always login with this, if they knew it.
</details>
<details>
<summary>fix: slot enable/disable for HPC cluster (#785)</summary>
fix: slot enable/disable for HPC cluster (#785)
</details>
<details>
<summary>fix: Show container started messages in exp log [DET-9285] (#796)</summary>
fix: Show container started messages in exp log [DET-9285] (#796)

Recursion is seeing occasional delayed start of containers on specific nodes.
We have messages about container startup, but they are in the master log.
This moves them into the exp log where they are visible and easily found
to assist in identifying problematic container startup.  They are shown
only when there are mulitple containers involved.
</details>
<details>
<summary>fix: Show more lines when containers haven't started [DET-9285] (#797)</summary>
fix: Show more lines when containers haven't started [DET-9285] (#797)

A 2nd part of the problem is when a download is slow and eventually fails
why has it failed, or were there any errors that caused it.
This increment will show up to 500 lines of error log output, but only
if the containers have not all started.   Once the containers start, then
we retain the current limits of most recent 15 lines.
</details>
<details>
<summary>fix(dispatcherrm): ResolveResourcePool impl is semantically wrong (#7…</summary>
fix(dispatcherrm): ResolveResourcePool impl is semantically wrong (#799) [DET-9286]

In OSS Determined, a fix was landed so that we resolve task container defaults with the
correct resource pool but, because DispatcherRM's ResolveResourcePool impl was slightly
wrong, we were accidentally relying on the incorrect behavior to end up with the right
defaults. The callers of ResolveResourcePool do no expect to receive 'providing partition'
but just to have defaults applied, because of this we got the wrong task container defaults
and it caused the runtime expconf contain the wrong pool.
</details>
<details>
<summary>fix(dispatcherrm): do not remerge task_container_defaults.sbatch_args…</summary>
fix(dispatcherrm): do not remerge task_container_defaults.sbatch_args (#800) [DET-9261]

Because we merge the fully-resolved task container defaults into the expconf for a
trial and default command config for commands, we don't need to remerge
task_container_defaults.sbatch_args here.
</details>
<details>
<summary>fix(dispatcherrm): do not skip partition overrides when there is a cu…</summary>
fix(dispatcherrm): do not skip partition overrides when there is a custom pool (#801) [DET-9262]
</details>
<details>
<summary>fix: FilterExperimentsQuery ignoring assigned groups (#809)</summary>
fix: FilterExperimentsQuery ignoring assigned groups (#809)
</details>
<details>
<summary>fix: Need to garbage collect orphaned dispatches [FOUNDENG-535] (#808)</summary>
fix: Need to garbage collect orphaned dispatches [FOUNDENG-535] (#808)

On error paths, determined may not cleanup dispatches leading to orphans. We need to periodically cleanup old launches to avoid the overhead of lots of orphaned which causes overhead when using the search APIs (/running, /terminated).
</details>
<details>
<summary>fix: reload auth token on 403 status code from the launcher (#814)</summary>
fix: reload auth token on 403 status code from the launcher (#814)
</details>
<details>
<summary>fix: Avoid excessive dispatches in debugging mode [FOUNDENG-567] (#817)</summary>
fix: Avoid excessive dispatches in debugging mode [FOUNDENG-567] (#817)

When the master is in debug mode we leave dispatches on the disk for triage purposes.
This change adds a periodic cleanup every 18hr in debug mode using the existing
inactive dispatch cleanup method.   We do one immediate cleanup on startup then
if in debug mode, we continue to periodically repeat the cleanup.
</details>
<details>
<summary>fix: Log response msg when loadEnvironmentLog failed (#818)</summary>
fix: Log response msg when loadEnvironmentLog failed (#818)
</details>
<details>
<summary>fix: GC jobs do not inherit sbatch_args from custom resource_pool [DE…</summary>
fix: GC jobs do not inherit sbatch_args from custom resource_pool [DET-9363] (#827)

Removed the appending of 't.TaskContainerDefaults.Pbs.SbatchArgs()' in dispatcher_task.go, as it results in duplicate entries
</details>
<details>
<summary>fix: Moving experiment checks workspace id from dest. project (not ex…</summary>
fix: Moving experiment checks workspace id from dest. project (not exp) (#832)
</details>
<details>
<summary>fix: export envs for slotType (#840)</summary>
fix: export envs for slotType (#840)
</details>
<details>
<summary>fix: fix bug with launching tensorboards on trials (#842)</summary>
fix: fix bug with launching tensorboards on trials (#842)

fix SQL query causing tensorboards with explicitly requested trials to fail to launch.

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Leverage assigned dispatchID in DispatchRM [DET-9474] (#847)</summary>
fix: Leverage assigned dispatchID in DispatchRM [DET-9474] (#847)

Utilize the 3.2.9 launcher feature to choose the dispatchID at launch
to simplify the detection of throttled job launches.  Use the
allocationID as the dispatcheID, and switch to synchronous launch.
</details>
<details>
<summary>fix: Tighten up status check on delayed launch [DET-9474] (#854)</summary>
fix: Tighten up status check on delayed launch [DET-9474] (#854)

If the synchronous launch takes too long, we may run into a
job monitoring poll which will detect the dispatchID as 404/NOT_FOUND
and stop monitoring. This would cause status updates to stop and
the job to show only QUEUED even though it is executing.
Add a launchInProgress flag to the job monitor to cause 404/NOT_FOUND
to be ignored until the launch has succeeded to avoid this case.
</details>
<details>
<summary>fix: Ensure exit sent on lost dispatch [FE-3] (#857)</summary>
fix: Ensure exit sent on lost dispatch [FE-3] (#857)

If a dispatch suddenly disappeared from the luancher (404) without any action
monitoring was dropped without any notification of job completion.
On 404, notify that the job was lost and terminate.
</details>
<details>
<summary>fix: Suppress incorrect error message on launch fail [FE-5] (#860)</summary>
fix: Suppress incorrect error message on launch fail [FE-5] (#860)

If we fail to get the assigned dispatchID we report an error about requiring
launcher 3.2.9.  On a failed launch, there may be no dispatchID assigned
at all so we should check the dispatchID only on successful launches.
</details>
<details>
<summary>fix: Improve error message formatting with synchronous launch [FE-5] …</summary>
fix: Improve error message formatting with synchronous launch [FE-5] (#862)

After converting from LaunchAsync to synchronous Launch, we now have
a different path for processing failed launches.   Previously this was
handled by the dispatch_monitor, but now we get back the error as a direct
REST call response.  So handle BadRequest and InternalServerError directly
from the detail message from the response body (if available), instead of
showing the whole error response in JSON.

All the Slurm failure tests still succeeded, but the messages were surrounded
by the additional JSON fields which made it much less appealing.
</details>
<details>
<summary>fix: Cleaup checkpoints and launcher state from prior runs [DET-9504]…</summary>
fix: Cleaup checkpoints and launcher state from prior runs [DET-9504] (#864)

- The checkpoints tend to grow forever.   Remove prior checkpoints to avoid filling up /scratch
- Remove the jobs & archiveVolumes from job_storage_root so that re-used IDs from prior runs (due to test configuation) will not conflict.
</details>
<details>
<summary>fix: handle launcher DispatchState.MISSING (#872)</summary>
fix: handle launcher DispatchState.MISSING (#872)
</details>
<details>
<summary>fix: allow oidc users without scim [DET-8786] (#889)</summary>
fix: allow oidc users without scim [DET-8786] (#889)
</details>
<details>
<summary>fix: make rebranding less brittle</summary>
fix: make rebranding less brittle
</details>
<details>
<summary>fix: Modify "slurmcluster.sh" to create the SOCKS5 proxy SSH tunnel a…</summary>
fix: Modify "slurmcluster.sh" to create the SOCKS5 proxy SSH tunnel and export the ALL_PROXY environment variable [FE-70] (#904)
</details>
<details>
<summary>fix: Make changes to get "make slurmcluster" shell tests to work [FE-…</summary>
fix: Make changes to get "make slurmcluster" shell tests to work [FE-74] (#907)

* fix: Make changes to get "make slurmcluster" shell tests to work [FE-74]

* Updated comment
</details>
<details>
<summary>fix: DET-9483 successfully run e2e_slurm_preemption tests as part of …</summary>
fix: DET-9483 successfully run e2e_slurm_preemption tests as part of nightly workflow (#903)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: DET-9624. Add -error.log to exp logs (#920)</summary>
fix: DET-9624. Add -error.log to exp logs (#920)

* fix: DET-9624. Add -error.log to exp logs
</details>
<details>
<summary>fix: Enable concurrent rocm runs on a node [FE-98] (#922)</summary>
fix: Enable concurrent rocm runs on a node [FE-98] (#922)

With the latest environments:rocm-4.5-pytorch-1.10-tf-2.10-rocm-0.21.2 builds
if both ROCR_VISIBLE_DEVICES and CUDA_VISIBLE_DEVICES are set and do not
include 0, then the error "No HIP GPUs are available" occurs.  Slurm
tends to provide both, so unset CUDA_VISIBLE_DEVICES when running ROCm
and ROCR_VISIBLE_DEVICES is set.
</details>
<details>
<summary>fix: ROCM GPUs not shown as in use (#932)</summary>
fix: ROCM GPUs not shown as in use (#932)

when slot_type in master set to CUDA
</details>
<details>
<summary>fix: FE-110: Determined master not displaying PBS errors on GCP PBS i…</summary>
fix: FE-110: Determined master not displaying PBS errors on GCP PBS image (#939)

* [ALLGCP] Enabled pbs job history in startup script so that the determined master logs will work properly

* [ALLGCP] Added tests back to pbs suites to see what fails

* [ALLGCP] Got rid of debugging requires: build-go in config.yaml

* [ALLGCP] Snuck in a change with top-level make unslurmcluster target that I previously broke
</details>
<details>
<summary>fix: Zero-slot tasks fail with an unexpected message (#938)</summary>
fix: Zero-slot tasks fail with an unexpected message (#938)
</details>
<details>
<summary>fix: correct some migration file names (#945)</summary>
fix: correct some migration file names (#945)

Correct the file names of some old scim & oauth EE migrations
so they end with `*.tx.{up,down}.sql`
</details>
<details>
<summary>fix: FE-116: CI fails test-e2e-*-enroot-gcp upon update of CPU image …</summary>
fix: FE-116: CI fails test-e2e-*-enroot-gcp upon update of CPU image version (#947)

* [ALLGCP] Added a way for the default CPU image to be checked and default to what is already there

* [ALLGCP] Added new images slurm and pbs

* [ALLGCP] Testing out of date image

* [ALLGCP] Added correct logic for when image is out of date

* [ALLGCP] Added correct logic for when image is out of date

* [ALLGCP] Testing with out-of-date images final

* [ALLGCP] Got rid of debugging in scripts

* [ALLGCP] Added a wait for VM creation step that is less trivial than a constant sleep

* [ALLGCP] Wait for VM creation jhob checks every 5 seconds now

* [ALLGCP] Changing formatting of wait for vm time

* [ALLGCP] Adding some more context to logging in slurmcluster.sh
</details>
<details>
<summary>fix: remove duplicated slurm/pbs sbatch args (#958)</summary>
fix: remove duplicated slurm/pbs sbatch args (#958)
</details>
<details>
<summary>fix: test-e2e-slurm-znode: Nightly test failure test_task_logs[tensor…</summary>
fix: test-e2e-slurm-znode: Nightly test failure test_task_logs[tensorboard-task_config3-] [FE-134] (#979)
</details>
<details>
<summary>fix: Nightly failure: test_task_logs[tensorboard-task_config3-^.*Tens…</summary>
fix: Nightly failure: test_task_logs[tensorboard-task_config3-^.*TensorBoard .* at .*$] -- Circleci config.yml cleanup [FE-143] (#980)

* fix: Nightly failure: test_task_logs[tensorboard-task_config3-^.*TensorBoard .* at .*$] -- Circleci config.yml cleanup [FE-143]

* Temporarily commenting out the 'triggers:' section from 'nightly:' to see if the znode tests that were previously failing now pass at the gate.

* Reverted the temporary commenting out of the 'triggers:' section from 'nightly:'

* Temporarily commented out the 'triggers:' section from 'nightly:' again, to see if the rebase with Canming's changes help reduce the failures

* Reverted the temporary commenting out of the 'triggers:' section from 'nightly:'
</details>
<details>
<summary>fix: update some host names (#984)</summary>
fix: update some host names (#984)
</details>
<details>
<summary>fix: temp fix for dispatcher RM before implementing External Jobs fea…</summary>
fix: temp fix for dispatcher RM before implementing External Jobs feature (#985)
</details>
<details>
<summary>fix: Active job across upgrade 0.21.0-ee->0.22.2-ee causes crash [FE-…</summary>
fix: Active job across upgrade 0.21.0-ee->0.22.2-ee causes crash [FE-156] (#998)

Prior to 0.22.2 the dispatchID was distinct from the allocationID and we
mapped between them.   In 0.22.2, we started using the allocationID
as the dispatchID.  What we missed is that active dispatches prior to
0.22.2 will not have this property so if we try to use the dispatchID
as the allocationID it will not find it.  After the upgrade to 0.22.2+
we lookup the AllocateRequest using the incorrect ID and it causes a
NPE.

This fix does the lookup of the AllocateRequest, and if it fails it
reverts to the old behavior of looking up the AllocationID from the
DB table.   This fix can be dropped when we stop supporting upgrades
from releases prior to 0.22.2
</details>
<details>
<summary>fix: dispatcher not guarding bound resource pools (#999)</summary>
fix: dispatcher not guarding bound resource pools (#999)
</details>
<details>
<summary>fix: Avoid potential NPE on delayed messages [FE-167] (#1002)</summary>
fix: Avoid potential NPE on delayed messages [FE-167] (#1002)

On Greenlake MLDEaaS deployment the file system used for job_storage_root
sometimes goes away for a bit.  This causes hangs, arbitrary launcher service
restarts, etc.   Occastionally, this causes dispatch monitoring events to be
delayed so long that other portions of the code have already detected the failure
and terminated the allocation.   When the DispatchStateChange eventually gets
delivered the task does not exist, and causes an NPE.   Ignore updates for
tasks that are no longer active.
</details>
<details>
<summary>fix: The Determined AI resource pools should only show slots for node…</summary>
fix: The Determined AI resource pools should only show slots for nodes that are in service [FE-164] (#1000)

* fix: The Determined AI resource pools should only show slots for nodes that are in service [FE-164]

* Fixed unit test

* Fixed lint issue

* Fixed typo in comment
</details>
<details>
<summary>fix: Add slot_type validation to slurm/pbs RM [DET-9739] (#1009)</summary>
fix: Add slot_type validation to slurm/pbs RM [DET-9739] (#1009)

An incorrect slot_type with resource_manager: slurm|pbs was
not reported until used.   Add a resource manager config validation
to catch the failure at startup of the master.
</details>
<details>
<summary>fix: allow dispatch_gc delete dispatch without name (#1008)</summary>
fix: allow dispatch_gc delete dispatch without name (#1008)
</details>
<details>
<summary>fix: hot fix for recent RBAC changes from OSS (#1015)</summary>
fix: hot fix for recent RBAC changes from OSS (#1015)
</details>
<details>
<summary>fix: Avoid non-distributed jobs from getting resources from multiple …</summary>
fix: Avoid non-distributed jobs from getting resources from multiple nodes. [FE-175] (#1014)

* fix: Avoid non-distributed jobs from getting resources from multiple nodes. [FE-175]

* Don't preserve existing instances for commands, shells, and notebooks, as 'per-node' and 'nodes' together will cause Slurm batch file to be rejected

* Made Bradley's suggested change in the review
</details>
<details>
<summary>fix: remove some temporary files (#1013)</summary>
fix: remove some temporary files (#1013)

* fix: remove some temporary files

The launcher REST API LoadEnvironmentLog yields a temporary file
that we need to remove once its content has been consumed.
</details>
<details>
<summary>fix: Test test_slurm_verify_home fails with podman and it shouldn't […</summary>
fix: Test test_slurm_verify_home fails with podman and it shouldn't [FE-136] (#1028)

Removed the skipping of 'test_slurm_verify_home' for podman from .circleci/config.yml [ALLGCP]

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: Change Determined master to abort on bad launcher version [FE-18…</summary>
fix: Change Determined master to abort on bad launcher version [FE-184] (#1031)

To be sure that Determined will not be used with an incompatible
launcher version, immediately abort the server during startup
if the launcher version is insufficient or we cannot communicate with
the launcher.    The determined-master.service currently only retries the
startup for 20secs, this will be increase to avoid potential false failures
if systemd starts determined-master.service before launcher.service.
</details>
<details>
<summary>fix: PBS with default_compute_resource_pool crashes [FE-215] (#1056)</summary>
fix: PBS with default_compute_resource_pool crashes [FE-215] (#1056)

The PBS check that was added for default resource pools referenced
the wrong variable after the null check resulting in an NPE.
</details>
<details>
<summary>fix: Counts for Active and Queued tabs on the resource pool page shou…</summary>
fix: Counts for Active and Queued tabs on the resource pool page should include external jobs (#1060)
</details>
<details>
<summary>fix: update rbac intg test call (#1074)</summary>
fix: update rbac intg test call (#1074)
</details>
<details>
<summary>fix: allow org admins to bypass other checks in MLDES authentication …</summary>
fix: allow org admins to bypass other checks in MLDES authentication (#1072)
</details>
<details>
<summary>fix: Update the GetJobQueueStatsRequest method to use the GetResource…</summary>
fix: Update the GetJobQueueStatsRequest method to use the GetResourcePools method to include the custom resource pools. (#1077)
</details>
<details>
<summary>fix: Update variable name for OICD client secret (#1099)</summary>
fix: Update variable name for OICD client secret (#1099)

* update envvar name in service.go

* update oidc client secret name in master-deployment
</details>
<details>
<summary>fix: return 401 status code instead of 500 for oauth permission failu…</summary>
fix: return 401 status code instead of 500 for oauth permission failures (#1082)
</details>
<details>
<summary>fix: removes workspace edit permission from editor role (DET-9909) (#…</summary>
fix: removes workspace edit permission from editor role (DET-9909) (#1114)
</details>
<details>
<summary>fix: return correct location URL for /Users SCIM API endpoint (#1115)</summary>
fix: return correct location URL for /Users SCIM API endpoint (#1115)

[excluding e2e_tests changes]
</details>
<details>
<summary>fix: EE returns information about SSO Provider type to Web (#1124)</summary>
fix: EE returns information about SSO Provider type to Web (#1124)

* re-apply detcloud commit
</details>
<details>
<summary>fix: replace antd select with hew select (#8424)</summary>
fix: replace antd select with hew select (#8424)

* fix: replace antd select with hew select

* fix: hp search select width

* test: fix hyper parameter test cases

* fix: minor fix

(cherry picked from commit 1041e560bc5cb797759e7129915d24b7b5341f7b)
</details>
<details>
<summary>fix: bash bug reading unset DET_DEBUG (#1131)</summary>
fix: bash bug reading unset DET_DEBUG (#1131)

(cherry picked from commit 3891cda28a5a2e1bfaa1533786027dffcd3e180d)
</details>
<details>
<summary>fix: Wrap older modals in theme class [WEB-1824] (#8432)</summary>
fix: Wrap older modals in theme class [WEB-1824] (#8432)

antd.Modal in all but one known cases

(cherry picked from commit bf07e6146189c1991d3fc82325e97e3618496978)
</details>
<details>
<summary>fix: don't dup logs on singluarity and podman (#1130)</summary>
fix: don't dup logs on singluarity and podman (#1130)

The old log shippper used strtobool and so a "=False" env var would
evaluate to False.  The new log shipper treats non-empty env vars as
True.  This was the source of the bug.

However, I see no reason why we can't let the user's setting for
DET_EMIT_STDOUT_LOGS be definitive.  I don't know why they would set it
here but it feels like it should be their prerogative.

(cherry picked from commit 48371be89f6aaa783cee68405b4d7ead897e1895)
</details>
<details>
<summary>fix: new experiment list tooltip styling (#8433)</summary>
fix: new experiment list tooltip styling (#8433)

(cherry picked from commit 292c75d6035de2cd57c79e9d30ba8d6ba2d42006)
</details>
<details>
<summary>fix: k8s autoscaling nodes not counted towards RP (#8439)</summary>
fix: k8s autoscaling nodes not counted towards RP (#8439)

(cherry picked from commit e9a199ae3e3321335cb10e01b86f0106fa0776b8)
</details>
<details>
<summary>fix: Calculate allocation bar stats same as overview [WEB-1822] (#8431)</summary>
fix: Calculate allocation bar stats same as overview [WEB-1822] (#8431)

(cherry picked from commit cde18df81f955dc60028eddd7120aca35fe3cd7c)
</details>
